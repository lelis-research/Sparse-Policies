from agent import PolicyGuidedAgent
import numpy as np
import math, copy


class LevinLossMLP:
    def is_mlp_applicable(self, trajectory, actions, j):
        """
        This function checks whether an MLP is applicable in a given state. 

        An MLP is applicable if the sequence of actions it produces matches
        the sequence of actions in the trajectory. Note that we do not consider an
        MLP if it has less than 2 actions, as it would be equivalent to a 
        primitive action. 
        """
        if len(actions) <= 1 or len(actions) + j > len(trajectory):
            return False
        
        for i in range(len(actions)):
            if actions[i] != trajectory[i + j][1]:
                return False
        return True

    def _run(self, env, mask, model, numbers_steps):
        """
        This function executes an option, which is given by a mask, a model, and a number of steps. 

        It runs the masked model for the specified number of steps and it returns the actions taken for those steps. 
        """
        agent = PolicyGuidedAgent()
        trajectory = agent.run_with_mask(env, model, mask, numbers_steps)

        actions = []
        for _, action in trajectory.get_trajectory():
            actions.append(action)

        return actions
    
    def _run_opt(self, env, model_opt, number_steps):
        """
        This function executes an option, which is given by a neural network model and a number of steps. 

        It runs the model for the specified number of steps and it returns the actions taken for those steps. 
        """
        agent = PolicyGuidedAgent()
        trajectory = agent.run(env, model_opt, length_cap=number_steps-1)

        actions = []
        for _, action in trajectory.get_trajectory():
            actions.append(action)

        return actions

    def _run_y1_y2(self, env, model_y1, model_y2, number_steps):
            """
            This function executes an option, which is given by two neural network models (model_y1 and model_y2)
            and a number of steps. It runs the models for the specified number of steps and returns the actions taken for those steps.
            """
            agent = PolicyGuidedAgent()

            # Run the models for the specified number of steps using agent
            trajectory = agent.run_with_y1_y2(env, model_y1, model_y2, length_cap=number_steps-1)

            actions = []
            for _, action in trajectory.get_trajectory():
                actions.append(action)

            return actions

    def loss(self, masks, models, trajectory, number_actions, joint_problem_name_list, problem_mlp, number_steps):
        """
        This function implements the dynamic programming method from Alikhasi & Lelis (2024). 

        Note that the if-statement with the following code is in a different place. I believe there is
        a bug in the pseudocode of Alikhasi & Lelis (2024).

        M[j] = min(M[j - 1] + 1, M[j])
        """
        t = trajectory.get_trajectory()
        M = np.arange(len(t) + 1)

        for j in range(len(t) + 1):
            if j > 0:
                M[j] = min(M[j - 1] + 1, M[j])
            if j < len(t):
                for i in range(len(masks)):
                    # the mask being considered for selection cannot be evaluated on the trajectory
                    # generated by the MLP trained to solve the problem.
                    if joint_problem_name_list[j] == problem_mlp:
                        continue
                    actions = self._run(copy.deepcopy(t[j][0]), masks[i], models[i], number_steps)

                    if self.is_mlp_applicable(t, actions, j):
                        M[j + len(actions)] = min(M[j + len(actions)], M[j] + 1)
        uniform_probability = (1/(len(masks) + number_actions)) 
        depth = len(t) + 1
        number_decisions = M[len(t)]

        # use the Levin loss in log space to avoid numerical issues
        log_depth = math.log(depth)
        log_uniform_probability = math.log(uniform_probability)
        return log_depth - number_decisions * log_uniform_probability
    
    def loss_opt(self, model_opts, trajectory, number_actions, joint_problem_name_list, problem_mlp, number_steps):
        """
        This function implements the dynamic programming method from Alikhasi & Lelis (2024). 

        Note that the if-statement with the following code is in a different place. I believe there is
        a bug in the pseudocode of Alikhasi & Lelis (2024).

        M[j] = min(M[j - 1] + 1, M[j])
        """
        t = trajectory.get_trajectory()
        M = np.arange(len(t) + 1)

        for j in range(len(t) + 1):
            if j > 0:
                M[j] = min(M[j - 1] + 1, M[j])
            if j < len(t):
                for i in range(len(model_opts)):
                    # the model_opt being considered for selection cannot be evaluated on the trajectory
                    # generated by the MLP trained to solve the problem.
                    if joint_problem_name_list[j] == problem_mlp:
                        continue
                    actions = self._run_opt(copy.deepcopy(t[j][0]), model_opts[i], number_steps)

                    if self.is_mlp_applicable(t, actions, j):
                        M[j + len(actions)] = min(M[j + len(actions)], M[j] + 1)
        uniform_probability = (1/(len(model_opts) + number_actions)) 
        depth = len(t) + 1
        number_decisions = M[len(t)]

        # use the Levin loss in log space to avoid numerical issues
        log_depth = math.log(depth)
        log_uniform_probability = math.log(uniform_probability)
        return log_depth - number_decisions * log_uniform_probability

    def loss_y1_y2(self, options_list, trajectory, number_actions, joint_problem_name_list, problem_mlp, number_steps):
        """
        This function implements the dynamic programming method from Alikhasi & Lelis (2024), 
        adapted for options that contain two models: model_y1 and model_y2.
        """
        t = trajectory.get_trajectory()
        M = np.arange(len(t) + 1)

        for j in range(len(t) + 1):
            if j > 0:
                M[j] = min(M[j - 1] + 1, M[j])
            if j < len(t):
                for i, option in enumerate(options_list):
                    # The option being considered for selection cannot be evaluated on the trajectory
                    # generated by the MLP trained to solve the problem.
                    if joint_problem_name_list[j] == problem_mlp:
                        continue

                    # Run the option's model_y1 and model_y2 on the environment
                    actions = self._run_y1_y2(copy.deepcopy(t[j][0]), option.model_y1, option.model_y2, number_steps)

                    if self.is_mlp_applicable(t, actions, j):
                        M[j + len(actions)] = min(M[j + len(actions)], M[j] + 1)

        uniform_probability = (1 / (len(options_list) + number_actions))
        depth = len(t) + 1
        number_decisions = M[len(t)]

        # Use the Levin loss in log space to avoid numerical issues
        log_depth = math.log(depth)
        log_uniform_probability = math.log(uniform_probability)
        return log_depth - number_decisions * log_uniform_probability
    
    def compute_loss(self, masks, models, problem_mlp, trajectories, number_actions, number_steps):
        """
        This function computes the Levin loss of a set of masks (programs). Each mask in the set is 
        what we select as a set of options, according to Alikhasi & Lelis (2024). 

        The loss is computed for a set of trajectories, one for each training task. Instead of taking
        the average loss across all trajectories, in this function we stich all trajectories together
        forming one long trajectory. The function is implemented this way for debugging purposes. 
        Since a mask k extracted from MLP b cannot be evaluated in the trajectory
        b generated, this "leave one out" was more difficult to debug. Stiching all trajectories
        into a single one makes it easier (see chained_trajectory below). 

        We still do not evaluate a mask on the data it was used to generate it. This is achieved
        with the vector joint_problem_name_list below, which is passed to the loss function. 
        """
        chained_trajectory = None
        joint_problem_name_list = []
        for problem, trajectory in trajectories.items():

            if chained_trajectory is None:
                chained_trajectory = copy.deepcopy(trajectory)
            else:
                chained_trajectory._sequence = chained_trajectory._sequence + copy.deepcopy(trajectory._sequence)
            name_list = [problem for _ in range(len(trajectory._sequence))]
            joint_problem_name_list = joint_problem_name_list + name_list
        return self.loss(masks, models, chained_trajectory, number_actions, joint_problem_name_list, problem_mlp, number_steps)
    
    def compute_loss_opt(self, model_opts, problem_mlp, trajectories, number_actions, number_steps):
        """
        This function computes the Levin loss of a set of model_opts (options). Each model_opt in the set is 
        what we select as a set of options, according to Alikhasi & Lelis (2024). 

        The loss is computed for a set of trajectories, one for each training task. Instead of taking
        the average loss across all trajectories, in this function we stich all trajectories together
        forming one long trajectory. The function is implemented this way for debugging purposes. 
        Since a model_opt extracted from MLP b cannot be evaluated in the trajectory
        b generated, this "leave one out" was more difficult to debug. Stiching all trajectories
        into a single one makes it easier (see chained_trajectory below). 

        We still do not evaluate a model_opt on the data it was used to generate it. This is achieved
        with the vector joint_problem_name_list below, which is passed to the loss function. 
        """
        chained_trajectory = None
        joint_problem_name_list = []
        for problem, trajectory in trajectories.items():

            if chained_trajectory is None:
                chained_trajectory = copy.deepcopy(trajectory)
            else:
                chained_trajectory._sequence = chained_trajectory._sequence + copy.deepcopy(trajectory._sequence)
            name_list = [problem for _ in range(len(trajectory._sequence))]
            joint_problem_name_list = joint_problem_name_list + name_list
        return self.loss_opt(model_opts, chained_trajectory, number_actions, joint_problem_name_list, problem_mlp, number_steps)

    def compute_loss_y1_y2(self, options_list, problem_mlp, trajectories, number_actions, number_steps):
        """
        This function computes the Levin loss of a set of options (model_y1 and model_y2 models in each option).
        It stitches all trajectories together into a single long trajectory to compute the loss.
        """
        chained_trajectory = None
        joint_problem_name_list = []
        for problem, trajectory in trajectories.items():
            if chained_trajectory is None:
                chained_trajectory = copy.deepcopy(trajectory)
            else:
                chained_trajectory._sequence = chained_trajectory._sequence + copy.deepcopy(trajectory._sequence)

            name_list = [problem for _ in range(len(trajectory._sequence))]
            joint_problem_name_list = joint_problem_name_list + name_list

        return self.loss_y1_y2(options_list, chained_trajectory, number_actions, joint_problem_name_list, problem_mlp, number_steps)

    def print_output_subpolicy_trajectory(self, models, masks, masks_problems, trajectories, number_steps):
        """
        This function prints the "behavior" of the options encoded in a set of masks. It will show
        when each option is applicable in different states of the different trajectories. Here is 
        a typical output of this function.

        BL-TR
        Mask:  o0
        001001102102001102001102
        -----000----------------
        --------------000-------
        --------------------000-

        Mask:  o3
        001001102102001102001102
        ------333---------------
        ---------------333------
        ----------------333-----
        ---------------------333
        ----------------------33

        Number of Decisions:  18

        It shows how different masks are used in a given sequence. In the example above, option o0
        is used in the sequence 110, while option o3 is used in some of the occurrences of 102. 
        """
        for problem, trajectory in trajectories.items():  
            print(problem)

            mask_usage = {}
            t = trajectory.get_trajectory()
            M = np.arange(len(t) + 1)

            for j in range(len(t) + 1):
                if j > 0:
                    if M[j - 1] + 1 < M[j]:
                        M[j] = M[j - 1] + 1

                if j < len(t):
                    for i in range(len(masks)):

                        if masks_problems[i] == problem:
                            continue

                        actions = self._run(copy.deepcopy(t[j][0]), masks[i], models[i], number_steps)

                        if self.is_mlp_applicable(t, actions, j):
                            M[j + len(actions)] = min(M[j + len(actions)], M[j] + 1)

                            mask_name = 'o' + str(i)
                            if mask_name not in mask_usage:
                                mask_usage[mask_name] = []

                            usage = ['-' for _ in range(len(t))]
                            for k in range(j, j+len(actions)):
                                usage[k] = str(i)
                            mask_usage[mask_name].append(usage)

            for mask, matrix in mask_usage.items():
                print('Mask: ', mask)
                for _, action in t:
                    print(action, end="")
                print()
                for use in matrix:
                    for v in use:
                        print(v, end='')
                    print()
                print()
            print('Number of Decisions: ',  M[len(t)])

    def print_output_subpolicy_trajectory_opt(self, model_opts, models_problems, trajectories, number_steps):
        """
        This function prints the "behavior" of the options encoded in a set of models. It will show
        when each option is applicable in different states of the different trajectories. Here is 
        a typical output of this function.

        BL-TR
        Model Option:  o0
        001001102102001102001102
        -----000----------------
        --------------000-------
        --------------------000-

        Model Option:  o3
        001001102102001102001102
        ------333---------------
        ---------------333------
        ----------------333-----
        ---------------------333
        ----------------------33

        Number of Decisions:  18

        It shows how different model_opts are used in a given sequence. In the example above, option o0
        is used in the sequence 110, while option o3 is used in some of the occurrences of 102. 
        """
        for problem, trajectory in trajectories.items():  
            print(problem)

            model_opt_usage = {}
            t = trajectory.get_trajectory()
            M = np.arange(len(t) + 1)

            for j in range(len(t) + 1):
                if j > 0:
                    if M[j - 1] + 1 < M[j]:
                        M[j] = M[j - 1] + 1

                if j < len(t):
                    for i in range(len(model_opts)):

                        if models_problems[i] == problem:
                            continue

                        actions = self._run_opt(copy.deepcopy(t[j][0]), model_opts[i], number_steps)

                        if self.is_mlp_applicable(t, actions, j):
                            M[j + len(actions)] = min(M[j + len(actions)], M[j] + 1)

                            model_opt_name = 'o' + str(i)
                            if model_opt_name not in model_opt_usage:
                                model_opt_usage[model_opt_name] = []

                            usage = ['-' for _ in range(len(t))]
                            for k in range(j, j+len(actions)):
                                usage[k] = str(i)
                            model_opt_usage[model_opt_name].append(usage)

            for model_opt, matrix in model_opt_usage.items():
                print('Model Option: ', model_opt)
                for _, action in t:
                    print(action, end="")
                print()
                for use in matrix:
                    for v in use:
                        print(v, end='')
                    print()
                print()
            print('Number of Decisions: ',  M[len(t)])

    def print_output_subpolicy_trajectory_opt(self, options_list, models_problems, trajectories, number_steps):
        """
        This function prints the "behavior" of the options encoded in a set of options. 
        It shows when each option is applicable in different states of the different trajectories.
        """
        for problem, trajectory in trajectories.items():
            print(problem)

            model_opt_usage = {}
            t = trajectory.get_trajectory()
            M = np.arange(len(t) + 1)

            for j in range(len(t) + 1):
                if j > 0:
                    if M[j - 1] + 1 < M[j]:
                        M[j] = M[j - 1] + 1

                if j < len(t):
                    for i, option in enumerate(options_list):
                        if models_problems[i] == problem:
                            continue

                        actions = self._run_y1_y2(copy.deepcopy(t[j][0]), option.model_y1, option.model_y2, number_steps)

                        if self.is_mlp_applicable(t, actions, j):
                            M[j + len(actions)] = min(M[j + len(actions)], M[j] + 1)

                            model_opt_name = 'o' + str(i)
                            if model_opt_name not in model_opt_usage:
                                model_opt_usage[model_opt_name] = []

                            usage = ['-' for _ in range(len(t))]
                            for k in range(j, j + len(actions)):
                                usage[k] = str(i)
                            model_opt_usage[model_opt_name].append(usage)

            for model_opt, matrix in model_opt_usage.items():
                print('Model Option: ', model_opt)
                for _, action in t:
                    print(action, end="")
                print()
                for use in matrix:
                    for v in use:
                        print(v, end='')
                    print()
                print()
            print('Number of Decisions: ', M[len(t)])