import copy
import itertools
import math
import random
import torch
from agent import PolicyGuidedAgent
from combo import Game
from model import CustomRelu
from agent import Trajectory


import numpy as np

class LevinLossMLP:
    def is_mlp_applicable(self, trajectory, actions, j):
        """
        This function checks whether an MLP is applicable in a given state. 

        An MLP is applicable if the sequence of actions it produces matches
        the sequence of actions in the trajectory. Note that we do not consider an
        MLP if it has less than 2 actions, as it would be equivalent to a 
        primitive action. 
        """
        if len(actions) <= 1 or len(actions) + j > len(trajectory):
            return False
        
        for i in range(len(actions)):
            if actions[i] != trajectory[i + j][1]:
                return False
        return True

    def _run(self, env, mask, model, numbers_steps):
        """
        This function executes an option, which is given by a mask, a model, and a number of steps. 

        It runs the masked model for the specified number of steps and it returns the actions taken for those steps. 
        """
        agent = PolicyGuidedAgent()
        trajectory = agent.run_with_mask(env, model, mask, numbers_steps)

        actions = []
        for _, action in trajectory.get_trajectory():
            actions.append(action)

        return actions
    
    def _run_opt(self, env, model_opt, number_steps):
        """
        This function executes an option, which is given by a neural network model and a number of steps. 

        It runs the model for the specified number of steps and it returns the actions taken for those steps. 
        """
        agent = PolicyGuidedAgent()
        trajectory = agent.run(env, model_opt, length_cap=number_steps-1)

        actions = []
        for _, action in trajectory.get_trajectory():
            actions.append(action)

        return actions

    def loss(self, masks, models, trajectory, number_actions, joint_problem_name_list, problem_mlp, number_steps):
        """
        This function implements the dynamic programming method from Alikhasi & Lelis (2024). 

        Note that the if-statement with the following code is in a different place. I believe there is
        a bug in the pseudocode of Alikhasi & Lelis (2024).

        M[j] = min(M[j - 1] + 1, M[j])
        """
        t = trajectory.get_trajectory()
        M = np.arange(len(t) + 1)

        for j in range(len(t) + 1):
            if j > 0:
                M[j] = min(M[j - 1] + 1, M[j])
            if j < len(t):
                for i in range(len(masks)):
                    # the mask being considered for selection cannot be evaluated on the trajectory
                    # generated by the MLP trained to solve the problem.
                    if joint_problem_name_list[j] == problem_mlp:
                        continue
                    actions = self._run(copy.deepcopy(t[j][0]), masks[i], models[i], number_steps)

                    if self.is_mlp_applicable(t, actions, j):
                        M[j + len(actions)] = min(M[j + len(actions)], M[j] + 1)
        uniform_probability = (1/(len(masks) + number_actions)) 
        depth = len(t) + 1
        number_decisions = M[len(t)]

        # use the Levin loss in log space to avoid numerical issues
        log_depth = math.log(depth)
        log_uniform_probability = math.log(uniform_probability)
        return log_depth - number_decisions * log_uniform_probability
    
    def loss_opt(self, model_opts, models, trajectory, number_actions, joint_problem_name_list, problem_mlp, number_steps):
        """
        This function implements the dynamic programming method from Alikhasi & Lelis (2024). 

        Note that the if-statement with the following code is in a different place. I believe there is
        a bug in the pseudocode of Alikhasi & Lelis (2024).

        M[j] = min(M[j - 1] + 1, M[j])
        """
        t = trajectory.get_trajectory()
        M = np.arange(len(t) + 1)

        for j in range(len(t) + 1):
            if j > 0:
                M[j] = min(M[j - 1] + 1, M[j])
            if j < len(t):
                for i in range(len(model_opts)):
                    # the model_opt being considered for selection cannot be evaluated on the trajectory
                    # generated by the MLP trained to solve the problem.
                    if joint_problem_name_list[j] == problem_mlp:
                        continue
                    actions = self._run_opt(copy.deepcopy(t[j][0]), model_opts[i], number_steps)

                    if self.is_mlp_applicable(t, actions, j):
                        M[j + len(actions)] = min(M[j + len(actions)], M[j] + 1)
        uniform_probability = (1/(len(model_opts) + number_actions)) 
        depth = len(t) + 1
        number_decisions = M[len(t)]

        # use the Levin loss in log space to avoid numerical issues
        log_depth = math.log(depth)
        log_uniform_probability = math.log(uniform_probability)
        return log_depth - number_decisions * log_uniform_probability

    def compute_loss(self, masks, models, problem_mlp, trajectories, number_actions, number_steps):
        """
        This function computes the Levin loss of a set of masks (programs). Each mask in the set is 
        what we select as a set of options, according to Alikhasi & Lelis (2024). 

        The loss is computed for a set of trajectories, one for each training task. Instead of taking
        the average loss across all trajectories, in this function we stich all trajectories together
        forming one long trajectory. The function is implemented this way for debugging purposes. 
        Since a mask k extracted from MLP b cannot be evaluated in the trajectory
        b generated, this "leave one out" was more difficult to debug. Stiching all trajectories
        into a single one makes it easier (see chained_trajectory below). 

        We still do not evaluate a mask on the data it was used to generate it. This is achieved
        with the vector joint_problem_name_list below, which is passed to the loss function. 
        """
        chained_trajectory = None
        joint_problem_name_list = []
        for problem, trajectory in trajectories.items():

            if chained_trajectory is None:
                chained_trajectory = copy.deepcopy(trajectory)
            else:
                chained_trajectory._sequence = chained_trajectory._sequence + copy.deepcopy(trajectory._sequence)
            name_list = [problem for _ in range(len(trajectory._sequence))]
            joint_problem_name_list = joint_problem_name_list + name_list
        return self.loss(masks, models, chained_trajectory, number_actions, joint_problem_name_list, problem_mlp, number_steps)
    
    def compute_loss_opt(self, model_opts, models, problem_mlp, trajectories, number_actions, number_steps):
        """
        This function computes the Levin loss of a set of model_opts (options). Each model_opt in the set is 
        what we select as a set of options, according to Alikhasi & Lelis (2024). 

        The loss is computed for a set of trajectories, one for each training task. Instead of taking
        the average loss across all trajectories, in this function we stich all trajectories together
        forming one long trajectory. The function is implemented this way for debugging purposes. 
        Since a model_opt extracted from MLP b cannot be evaluated in the trajectory
        b generated, this "leave one out" was more difficult to debug. Stiching all trajectories
        into a single one makes it easier (see chained_trajectory below). 

        We still do not evaluate a model_opt on the data it was used to generate it. This is achieved
        with the vector joint_problem_name_list below, which is passed to the loss function. 
        """
        chained_trajectory = None
        joint_problem_name_list = []
        for problem, trajectory in trajectories.items():

            if chained_trajectory is None:
                chained_trajectory = copy.deepcopy(trajectory)
            else:
                chained_trajectory._sequence = chained_trajectory._sequence + copy.deepcopy(trajectory._sequence)
            name_list = [problem for _ in range(len(trajectory._sequence))]
            joint_problem_name_list = joint_problem_name_list + name_list
        return self.loss_opt(model_opts, models, chained_trajectory, number_actions, joint_problem_name_list, problem_mlp, number_steps)

    def print_output_subpolicy_trajectory(self, models, masks, masks_problems, trajectories, number_steps):
        """
        This function prints the "behavior" of the options encoded in a set of masks. It will show
        when each option is applicable in different states of the different trajectories. Here is 
        a typical output of this function.

        BL-TR
        Mask:  o0
        001001102102001102001102
        -----000----------------
        --------------000-------
        --------------------000-

        Mask:  o3
        001001102102001102001102
        ------333---------------
        ---------------333------
        ----------------333-----
        ---------------------333
        ----------------------33

        Number of Decisions:  18

        It shows how different masks are used in a given sequence. In the example above, option o0
        is used in the sequence 110, while option o3 is used in some of the occurrences of 102. 
        """
        for problem, trajectory in trajectories.items():  
            print(problem)

            mask_usage = {}
            t = trajectory.get_trajectory()
            M = np.arange(len(t) + 1)

            for j in range(len(t) + 1):
                if j > 0:
                    if M[j - 1] + 1 < M[j]:
                        M[j] = M[j - 1] + 1

                if j < len(t):
                    for i in range(len(masks)):

                        if masks_problems[i] == problem:
                            continue

                        actions = self._run(copy.deepcopy(t[j][0]), masks[i], models[i], number_steps)

                        if self.is_mlp_applicable(t, actions, j):
                            M[j + len(actions)] = min(M[j + len(actions)], M[j] + 1)

                            mask_name = 'o' + str(i)
                            if mask_name not in mask_usage:
                                mask_usage[mask_name] = []

                            usage = ['-' for _ in range(len(t))]
                            for k in range(j, j+len(actions)):
                                usage[k] = str(i)
                            mask_usage[mask_name].append(usage)

            for mask, matrix in mask_usage.items():
                print('Mask: ', mask)
                for _, action in t:
                    print(action, end="")
                print()
                for use in matrix:
                    for v in use:
                        print(v, end='')
                    print()
                print()
            print('Number of Decisions: ',  M[len(t)])

    def print_output_subpolicy_trajectory_opt(self, model_opts, models_problems, trajectories, number_steps):
        """
        This function prints the "behavior" of the options encoded in a set of models. It will show
        when each option is applicable in different states of the different trajectories. Here is 
        a typical output of this function.

        BL-TR
        Model Option:  o0
        001001102102001102001102
        -----000----------------
        --------------000-------
        --------------------000-

        Model Option:  o3
        001001102102001102001102
        ------333---------------
        ---------------333------
        ----------------333-----
        ---------------------333
        ----------------------33

        Number of Decisions:  18

        It shows how different model_opts are used in a given sequence. In the example above, option o0
        is used in the sequence 110, while option o3 is used in some of the occurrences of 102. 
        """
        for problem, trajectory in trajectories.items():  
            print(problem)

            model_opt_usage = {}
            t = trajectory.get_trajectory()
            M = np.arange(len(t) + 1)

            for j in range(len(t) + 1):
                if j > 0:
                    if M[j - 1] + 1 < M[j]:
                        M[j] = M[j - 1] + 1

                if j < len(t):
                    for i in range(len(model_opts)):

                        if models_problems[i] == problem:
                            continue

                        actions = self._run_opt(copy.deepcopy(t[j][0]), model_opts[i], number_steps)

                        if self.is_mlp_applicable(t, actions, j):
                            M[j + len(actions)] = min(M[j + len(actions)], M[j] + 1)

                            model_opt_name = 'o' + str(i)
                            if model_opt_name not in model_opt_usage:
                                model_opt_usage[model_opt_name] = []

                            usage = ['-' for _ in range(len(t))]
                            for k in range(j, j+len(actions)):
                                usage[k] = str(i)
                            model_opt_usage[model_opt_name].append(usage)

            for model_opt, matrix in model_opt_usage.items():
                print('Model Option: ', model_opt)
                for _, action in t:
                    print(action, end="")
                print()
                for use in matrix:
                    for v in use:
                        print(v, end='')
                    print()
                print()
            print('Number of Decisions: ',  M[len(t)])


def load_trajectories(problems, hidden_size, game_width):
    """
    This function loads one trajectory for each problem stored in variable "problems".

    The trajectories are returned as a dictionary, with one entry for each problem. 
    """
    trajectories = {}
    for problem in problems:
        env = Game(game_width, game_width, problem)
        agent = PolicyGuidedAgent()
        rnn = CustomRelu(game_width**2 * 2 + 9, hidden_size, 3)
        
        # rnn.load_state_dict(torch.load('binary/' + problem + '-relu-' + str(hidden_size) + '-model.pth'))
        rnn.load_state_dict(torch.load('binary/game-width' + str(game_width) + '-' + problem + '-relu-' + str(hidden_size) + '-model.pth'))

        trajectory = agent.run(env, rnn, greedy=True)
        trajectories[problem] = trajectory

    return trajectories

def evaluate_all_masks_for_model(masks, selected_models_of_masks, model, problem, trajectories, number_actions, number_iterations, hidden_size):
    """
    Function that evaluates all masks for a given model. It returns the best mask (the one that minimizes the Levin loss)
    for the current set of selected masks. It also returns the Levin loss of the best mask. 
    """
    values = [-1, 0, 1]

    best_mask = None
    best_value = None
    loss = LevinLossMLP()

    combinations = itertools.product(values, repeat=hidden_size)

    for value in combinations:
        current_mask = torch.tensor(value, dtype=torch.int8).view(1, -1)
        
        value = loss.compute_loss(masks + [current_mask], selected_models_of_masks + [model], problem, trajectories, number_actions, number_iterations)
        # print('Initial Mask: ', current_mask, best_value)

        if best_mask is None or value < best_value:
            best_value = value
            best_mask = copy.deepcopy(current_mask)
            print(best_mask, best_value)
                            
    return best_mask, best_value

def evaluate_all_masks_levin_loss():
    """
    This function implements the greedy approach for selecting masks (options) from Alikhasi and Lelis (2024).
    This method evaluates all possible masks of a given model and adds to the pool of options the one that minimizes
    the Levin loss. This process is repeated while we can minimize the Levin loss. 

    This method should only be used with small neural networks, as there are 3^n masks, where n is the number of neurons
    in the hidden layer. 
    """
    hidden_size = 6
    number_iterations = 3
    game_width = 5
    number_actions = 3
    problems = ["TL-BR", "TR-BL", "BR-TL", "BL-TR"]

    trajectories = load_trajectories(problems, hidden_size, game_width)

    previous_loss = None
    best_loss = None

    loss = LevinLossMLP()

    selected_masks = []
    selected_models_of_masks = []
    selected_options_problem = []

    while previous_loss is None or best_loss < previous_loss:
        previous_loss = best_loss

        best_loss = None
        best_mask = None
        model_best_mask = None
        problem_mask = None

        for problem in problems:
            print('Problem: ', problem)
            rnn = CustomRelu(game_width**2 * 2 + 9, hidden_size, 3)
            rnn.load_state_dict(torch.load('binary/game-width' + str(game_width) + '-' + problem + '-relu-' + str(hidden_size) + '-model.pth'))

            mask, levin_loss = evaluate_all_masks_for_model(selected_masks, selected_models_of_masks, rnn, problem, trajectories, number_actions, number_iterations, hidden_size)

            if best_loss is None or levin_loss < best_loss:
                best_loss = levin_loss
                best_mask = mask
                model_best_mask = rnn
                problem_mask = problem

                print('Best Loss so far: ', best_loss, problem)

        # we recompute the Levin loss after the automaton is selected so that we can use 
        # the loss on all trajectories as the stopping condition for selecting automata
        selected_masks.append(best_mask)
        selected_models_of_masks.append(model_best_mask)
        selected_options_problem.append(problem_mask)
        best_loss = loss.compute_loss(selected_masks, selected_models_of_masks, "", trajectories, number_actions, number_iterations)

        print("Levin loss of the current set: ", best_loss)

    # remove the last automaton added
    selected_masks = selected_masks[0:len(selected_masks) - 1]

    loss = LevinLossMLP()
    loss.print_output_subpolicy_trajectory(selected_models_of_masks, selected_masks, selected_options_problem, trajectories, number_iterations)

    # printing selected options
    for i in range(len(selected_masks)):
        print(selected_masks[i])

def hill_climbing(masks, selected_models_of_masks, model, problem, trajectories, number_actions, number_iterations, number_restarts, hidden_size):
    """
    Performs Hill Climbing in the mask space for a given model. Note that when computing the loss of a mask (option), 
    we pass the name of the problem in which the mask is used. That way, we do not evaluate an option on the problem in 
    which the option's model was trained. 

    Larger number of restarts will result in computationally more expensive searches, with the possibility of finding 
    a mask that better optimizes the Levin loss. 
    """
    best_mask = None
    values = [-1, 0, 1]
    best_overall = None
    best_value_overall = None
    loss = LevinLossMLP()

    for i in range(number_restarts):        
        value = random.choices(values, k=hidden_size)
        current_mask = torch.tensor(value, dtype=torch.int8).view(1, -1)

        best_value = loss.compute_loss(masks + [current_mask], selected_models_of_masks + [model], problem, trajectories, number_actions, number_iterations)
        # print('Initial Mask: ', current_mask, best_value)

        while True:
            made_progress = False
            for i in range(len(current_mask)):
                modifiable_current_mask = copy.deepcopy(current_mask)
                for v in values:
   
                    modifiable_current_mask[0][i] = v
                    eval_value = loss.compute_loss(masks + [modifiable_current_mask], selected_models_of_masks + [model], problem, trajectories, number_actions, number_iterations)

                    if best_mask is None or eval_value < best_value:
                        best_value = eval_value
                        best_mask = copy.deepcopy(modifiable_current_mask)

                        made_progress = True
                    
                current_mask = copy.deepcopy(best_mask)

            if not made_progress:
                break
        
        if best_overall is None or best_value < best_value_overall:
            best_overall = copy.deepcopy(best_mask)
            best_value_overall = best_value

            print('Best Mask Overall: ', best_overall, best_value_overall)
    return best_overall, best_value_overall

def hill_climbing_mask_space_training_data_levin_loss():
    """
    This function performs hill climbing in the space of masks of a ReLU neural network
    to minimize the Levin loss of a given data set. 
    """
    hidden_size = 32
    number_iterations = 3
    game_width = 5
    number_restarts = 100
    number_actions = 3
    problems = ["TL-BR", "TR-BL", "BR-TL", "BL-TR"]

    trajectories = load_trajectories(problems, hidden_size, game_width)

    previous_loss = None
    best_loss = None

    loss = LevinLossMLP()

    selected_options = []
    selected_models_of_masks = []
    selected_options_problem = []

    # the greedy loop of selecting options (masks)
    while previous_loss is None or best_loss < previous_loss:
        previous_loss = best_loss

        best_loss = None
        best_mask = None
        model_best_mask = None
        problem_mask = None

        for problem in problems:
            print('Problem: ', problem)
            rnn = CustomRelu(game_width**2 * 2 + 9, hidden_size, 3)
            rnn.load_state_dict(torch.load('binary/game-width' + str(game_width) + '-' + problem + '-relu-' + str(hidden_size) + '-model.pth'))

            mask, levin_loss = hill_climbing(selected_options, selected_models_of_masks, rnn, problem, trajectories, number_actions, number_iterations, number_restarts, hidden_size)

            if best_loss is None or levin_loss < best_loss:
                best_loss = levin_loss
                best_mask = mask
                model_best_mask = rnn
                problem_mask = problem
        print()

        # we recompute the Levin loss after the automaton is selected so that we can use 
        # the loss on all trajectories as the stopping condition for selecting masks
        selected_options.append(best_mask)
        selected_models_of_masks.append(model_best_mask)
        selected_options_problem.append(problem_mask)
        best_loss = loss.compute_loss(selected_options, selected_models_of_masks, "", trajectories, number_actions, number_iterations)

        print("Levin loss of the current set: ", best_loss)

    # remove the last automaton added
    selected_options = selected_options[0:len(selected_options) - 1]

    loss = LevinLossMLP()
    loss.print_output_subpolicy_trajectory(selected_models_of_masks, selected_options, selected_options_problem, trajectories, number_iterations)

    # printing selected options
    for i in range(len(selected_options)):
        print(selected_options[i])


def evaluate_all_options_for_model(selected_options, selected_models_of_options, model, problem, trajectories, number_actions, number_iterations, options_for_this_model):
    """
    Function that evaluates all options for a given model. It returns the best option (the one that minimizes the Levin loss)
    for the current set of selected options. It also returns the Levin loss of the best option. 
    """
    # values = [-1, 0, 1]

    best_option = None
    best_value = None
    loss = LevinLossMLP()

    # combinations = itertools.product(values, repeat=hidden_size)

    for current_option in options_for_this_model:
        # current_mask = torch.tensor(value, dtype=torch.int8).view(1, -1)
        
        value = loss.compute_loss_opt(selected_options + [current_option], selected_models_of_options + [model], problem, trajectories, number_actions, number_iterations)
        # print('Initial Mask: ', current_mask, best_value)
        print("Value: ", value)

        if best_option is None or value < best_value:
            best_value = value
            best_option = copy.deepcopy(current_option)
            print("Best Value: ", best_value)
                            
    return best_option, best_value


def evaluate_all_options_levin_loss(options, models, problems, trajectories):
    """
    This function implements the greedy approach for selecting options.
    This method evaluates all different options of a given model and adds to the pool of options the one that minimizes
    the Levin loss. This process is repeated while we can minimize the Levin loss.  
    """
    number_iterations = 3
    number_actions = 3

    previous_loss = None
    best_loss = None

    loss = LevinLossMLP()

    selected_options = []
    selected_models_of_options = []
    selected_options_problem = []

    while previous_loss is None or best_loss < previous_loss:
        previous_loss = best_loss

        best_loss = None
        best_mask = None
        model_best_option = None
        problem_mask = None

        # TODO: change "options of model". It works now because I only have 4 options (1 for each model/problem) but if there are more options for each problem it won't work anymore
        for problem, model, options_of_model in zip(problems, models, options):
            # TODO: is model correct here?
            print('Problem: ', problem)
            # rnn = CustomRelu(game_width**2 * 2 + 9, hidden_size, 3)
            # rnn.load_state_dict(torch.load('binary/game-width' + str(game_width) + '-' + problem + '-relu-' + str(hidden_size) + '-model.pth'))

            # mask, levin_loss = evaluate_all_masks_for_model(selected_masks, selected_models_of_masks, rnn, problem, trajectories, number_actions, number_iterations, hidden_size)
            option, levin_loss = evaluate_all_options_for_model(selected_options, selected_models_of_options, model, problem, trajectories, number_actions, number_iterations, [options_of_model])

            if best_loss is None or levin_loss < best_loss:
                best_loss = levin_loss
                best_mask = option
                model_best_option = model
                problem_mask = problem

                print('Best Loss so far: ', best_loss, problem)
            print("################################################ END PROBLEM")

        # we recompute the Levin loss after the automaton is selected so that we can use 
        # the loss on all trajectories as the stopping condition for selecting automata
        selected_options.append(best_mask)
        selected_models_of_options.append(model_best_option)
        selected_options_problem.append(problem_mask)
        best_loss = loss.compute_loss_opt(selected_options, selected_models_of_options, "", trajectories, number_actions, number_iterations)

        print("Levin loss of the current set: ", best_loss)
        print("################################################ END OPTION\n\n")


    # remove the last automaton added
    # selected_options = selected_options[0:len(selected_options) - 1]

    loss = LevinLossMLP()
    loss.print_output_subpolicy_trajectory_opt(selected_options, selected_options_problem, trajectories, number_iterations)

    # TODO: this is not a good wayccto print my options because they are neural networks.
    # printing selected options
    for i in range(len(selected_options)):
        print(selected_options[i])


def update_problem_seq_dict(seq_state_model_dict, model, trajectory, seq_len=3, stride=3):
    """
    This function updates the dictionary seq_state_model_dict with the information of a given trajectory.

    The format is like:
    problems_seq_dict = {
        problem1: {seq: [state1, state2, ...], seq2: seq: [state1, state2, ...]},
        problem2: {...},
        ...
    }
    """
    for i in range(0, len(trajectory.get_trajectory()) - seq_len + 1, stride):
        seq = tuple(trajectory.get_action_sequence()[i:i+seq_len])
        # print("seq: ", seq)

        # TODO: should I keep track of different stages of model when the state is not changing as well?
        state = trajectory.get_state_sequence()[i]
        seq_state_model_dict[seq] = seq_state_model_dict.get(seq, []) + [state]
        # print("seq_state_model_dict: ", seq_state_model_dict)

    return seq_state_model_dict


def create_trajectory(sequence_of_actions, states):
    trajectory = Trajectory()
    for action, state in zip(sequence_of_actions, states):
        trajectory.add_pair(state, action)
    return trajectory


def retrain_for_options(approach, models, problems_seq_dict):
    """
    This function creates an option for each problem by retrainin the model of that problem 
    on the similar sequences observed on that problem on other problems.

    Returns the list of options (retrained models)
    """
    modified_models = copy.deepcopy(models)
    problems_seq_cache = {problem: [] for problem in problems_seq_dict.keys()}
    for model, problem, idx in zip(modified_models, problems_seq_dict.keys(), range(len(modified_models))):
        for seq, states in problems_seq_dict[problem].items():
            # compare each sequence with the sequences in other problems and retrain the current model on those.
            if seq not in problems_seq_cache[problem]:
                for other_problem in problems_seq_dict.keys():
                    if other_problem != problem:
                        # re-train this model with similar sequences of actions from other problems
                        for seq_other, states_other in problems_seq_dict[other_problem].items():
                            if seq == seq_other:
                                problems_seq_cache[problem].append(seq)
                                if approach == "freeze":
                                    for _ in range(10):
                                        loss = model.train_freeze(create_trajectory(seq_other, states_other))
                                        # print("loss freeze", loss)
                                elif approach == "whole":
                                    for _ in range(10):
                                        loss = model.train(create_trajectory(seq_other, states_other))
                                        # print("loss", loss)

                        modified_models[idx] = model
    # print("PSC: ", problems_seq_cache)
    return modified_models


def retrain_super_option(approach, models, problems_seq_dict):
    """
    This function retrains an option (the first model) on all other models
    """
    super_option = models[0]
    base_problem = "TL-BR" # because we are using the first model as the base of our super option
    for problem in problems_seq_dict.keys():
        if problem != base_problem: # it is already trained on the base_problem
            for seq, states in problems_seq_dict[problem].items():
                if approach == "freeze":
                    for _ in range(10):
                        loss = super_option.train_freeze(create_trajectory(seq, states))
                elif approach == "whole":
                    for _ in range(10):
                        loss = super_option.train(create_trajectory(seq, states))

    return super_option
        

def test1_options_trajectories(test_models, problem_test, game_width, label, len_cap = 23):
    """
    This test is to see if the options are able to mix the actions from the models that are trained on in the test environment
    """
    for model, idx in zip(test_models, range(len(test_models))):
        print("\n",label, idx)
        env = Game(game_width, game_width, problem_test)
        agent = PolicyGuidedAgent()
        trajectory = agent.run(env, model, greedy=True, length_cap=len_cap)

        actions = trajectory.get_action_sequence()
        for i in range(len(actions)):
            if i % 3 == 0 and i != 0:
                print(" - ", end="")
            print(actions[i], end="")
        
    print("\n")


def test2_each_cell_grid(test_models, problem_test, game_width, label):
    """
    This test is to see for each cell, options will give which sequence of actions
    """
    for model, idx in zip(test_models, range(len(test_models))):
        print("\n",label, idx)
        for i in range(game_width):
            for j in range(game_width):    
                env = Game(game_width, game_width, problem_test, init_x=i, init_y=j)
                agent = PolicyGuidedAgent()
                trajectory = agent.run(env, model, greedy=True, length_cap=2)

                actions = trajectory.get_action_sequence()
                print("Cell: (", i, j, ") , ", actions)
                state = trajectory.get_state_sequence()[0]
                print(state.__repr__(actions))

            print("\n")
        
    print("#### ### ###\n")


def update_uniq_seq_dict(trajectory, model, window_size, stride=1, seq_dict=None):
    """
    The unique sequence dictionary is a dictionary that maps a sequence of actions to a tuple containing a model and a list of corresponding states.
    Parameters:
    - It takes a single trajectory and model as input.
    - It extracts the action sequence and state sequence from the trajectory.
    - It creates sliding windows of the action sequence with the specified window size and stride.
    - For each window, it checks if the sequence is already present in the dictionary.
    - If the sequence is not present, it adds the sequence as a key in the dictionary and associates it with the model and the corresponding state.
    - If the sequence is already present, it appends the corresponding state to the existing list of states.
    """
    actions = trajectory.get_action_sequence()
    states = trajectory.get_state_sequence()
    for i in range(0, len(actions) - window_size + 1, stride):
        seq = tuple(actions[i:i+window_size])
        if seq not in seq_dict:
            seq_dict[seq] = (model, [states[i]])
        else:
            seq_dict[seq][1].append(states[i])
    return seq_dict


def combinatorial_generalization(approach):
    """
    In this function, in phase 1, we keep track of the sequences of actions and the models that generate them.
    In the second phase, we re-train the models with the sequences of actions that occured in other problems.

    Re-training is done with two different approaches:
        Approach 1 (freeze): freeze the second layer and re-train the rest of the network
        Approach 2 (whole): re-train the entire network
    """

    hidden_size = 6
    game_width = 5
    problems = ["TL-BR", "TR-BL", "BR-TL", "BL-TR"]
    problem_test1 = "Test1"
    problem_test2 = "Test2"
    problems_seq_dict = {key: {} for key in problems}
    models = []
    uniq_seq_dict = {}

    rnn = CustomRelu(game_width**2 * 2 + 9, hidden_size, 3)


    # Phase 1: keeping track of sequences of actions and the models that generate them
    trajectories = load_trajectories(problems, hidden_size, game_width)
    for problem, trajectory in trajectories.items():
        rnn = CustomRelu(game_width**2 * 2 + 9, hidden_size, 3)
        rnn.load_state_dict(torch.load('binary/game-width' + str(game_width) + '-' + problem + '-relu-' + str(hidden_size) + '-model.pth'))
        models.append(rnn)

        print("Problem:", problem)
        print("actions: ", trajectory.get_action_sequence(), " \n")
        # print(rnn.print_weights(), " \n")
        # print("states: ", trajectory.get_state_sequence(), " \n")
        # print("Trajectory: \n", trajectory.get_trajectory(), " \n")

        window_sizes = list(range(2, len(trajectory.get_trajectory())))
        stride = 1

        for ws in window_sizes:
            # problems_seq_dict[problem] = update_problem_seq_dict(problems_seq_dict[problem], rnn, trajectory, seq_len=ws, stride=stride)
            uniq_seq_dict = update_uniq_seq_dict(trajectory, rnn, ws, stride=stride, seq_dict=uniq_seq_dict)



    ## Phase 2-1: re-training the models with the sequences of actions
    # options = [NN1, NN2, NN3, NN4]
    options = retrain_for_options(approach, copy.deepcopy(models), problems_seq_dict)
    # for idx in range(len(options)):
    #     print("new freeze weights: \n", options[idx].print_weights())
    #     print("old weights: \n", models[idx].print_weights())

    ## Phase 2-2: retrtaining an option that is trained on all other models
    # super_option = retrain_super_option(approach, copy.deepcopy(models), problems_seq_dict)


    ## Phase 3 - Test 1-1: Test the extracted options
    # test1_options_trajectories(models, problem_test1, game_width, label=approach + ": trajectory for model ")
    # test1_options_trajectories(options, problem_test1, game_width, label=approach + ": trajectory for option ")

    ## Test 1-2: Test the super option with base model 0 on all problems
    # test1_options_trajectories([super_option], problems[0], game_width, label=approach + ": trajectory for problem 'TL-BR' for super option ", len_cap = 23)
    # test1_options_trajectories([super_option], problems[1], game_width, label=approach + ": trajectory for problem 'TR-BL' for super option ", len_cap = 23)
    # test1_options_trajectories([super_option], problems[2], game_width, label=approach + ": trajectory for problem 'BR-TL' for super option ", len_cap = 23)
    # test1_options_trajectories([super_option], problems[3], game_width, label=approach + ": trajectory for problem 'BL-TR' for super option ", len_cap = 23)


    ## Test 2: Test the options on each cell of the grid and see the output of each option for it
    # test2_each_cell_grid(options, problem_test2, game_width, label=approach)


    ## Test 3: test the options using Mahdi's approach with Levin Loss
    evaluate_all_options_levin_loss(options, models, problems, trajectories)


def main():
    # evaluate_all_masks_levin_loss()
    # hill_climbing_mask_space_training_data_levin_loss()

    # combinatorial_generalization("freeze")
    combinatorial_generalization("whole")



if __name__ == "__main__":
    main()